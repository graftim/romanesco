Korpus und Preprocessing
Für diese Übung habe ich mir ein Korpus selbst erstellt. Ziel davon war es, ein Sprachmodell basierend auf Friedrich Nietzsches Werken zu trainieren. Dafür habe ich mir Nietzsches Werke «Also sprach Zarathustra», «Ecce Homo», «Jenseits von Gut und Böse», «Die Geburt der Tragödie» und «Götzen-Dämmerung» von Gutenberg heruntergeladen. Anschliessend habe ich die Dateien zusammengefügt und mit meinem preprocessing-skript «gutenberg_processor.py» bereinigt, sodass die meisten Noise-data entfernt wurden, und jeder Satz tokenisiert (mit NLTK) auf einer Zeile in mein Korpus-file geschrieben wurde. Somit hat das Korpus schon den Ansprüchen für den RNN-Trainingsinput genügt. Ich habe mich für ein Korpus mit Nietzsche-Werken entschieden, da seine Sätze doch sehr lang und zumindest für Menschen eher schwierig zu lesen sind. Ich hatte dementsprechend auch nicht so hohe Erwartungen and das RNN, aufgrund eben dieser Schwierigkeiten, sowie auch wegen dem ziemlich kleinen Korpus (1.6MB). 
Training
Die Hyperparameter für das Training habe ich auf den default-Werten belassen, ich habe allerdings in insgesamt 3 Trainings mit der Grösse des Hidden-layers gespielt. Im ersten Durchlauf habe ich am romanesco-code gar nichts verändert, womit ich eine Perplexität von 151 auf dem Trainingsset, respektive 153 auf dem Devset erreicht habe. Danach habe ich anstelle von 1500 Neuronen im Hidden-layer 2000 Neuronen verwendet, um zu sehen, ob ein grösserer Hiddenlayer allein die Resultate verbessert. Die Werte wurden ein wenig verbessert, im Training war die Perplexity noch 143, auf dem Devset 144. Zum Schluss habe ich dann den Code in romanesco verändert, sodass anstelle eines LSTMs ein GRU-RNN benutzt wurde. Für dieses RNN habe ich dann den Hiddenlayer auf 2000 Neuronen belassen. Ich denke, dass die 10 Epochen in diesem Fall ein wenig zu viel waren, denn während ich im Training eine Perplexity von 74 erreicht habe, war sie auf dem Devset bei 138, demnach hat das GRU RNN schneller overfitted als die LSTMs, aber trotzdem noch ein besseres Resultat erzielt. Meine Code-Anpassungen waren nicht so umfangreich oder schwierig, sodass ich dabei keine Probleme hatte.
